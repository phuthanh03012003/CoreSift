import os
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from transformers import ProphetNetTokenizer
import re

class Evaluation:
    def __init__(self, output_dir="./object"):
        self.output_dir = output_dir
        self.reference_path = os.path.join("..", "..", "..", "inputs", "TamCam.txt")
        self.result_file = os.path.join(self.output_dir, "evaluation_results.txt")

        # Táº£i tokenizer cá»§a ProphetNet
        self.tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased-cnndm")

        # XÃ³a ná»™i dung cÅ© náº¿u file Ä‘Ã£ tá»“n táº¡i
        with open(self.result_file, "w", encoding="utf-8") as file:
            file.write("**Tá»”NG Há»¢P Káº¾T QUáº¢ ÄÃNH GIÃ**\n")
            file.write("=" * 80 + "\n")

    def load_text(self, file_path):
        with open(file_path, "r", encoding="utf-8") as file:
            return file.read().strip()

    def clean_generated_text(self, text):
        # XÃ³a token Ä‘áº·c biá»‡t nhÆ° [X_SEP] Ä‘á»ƒ Ä‘áº£m báº£o Ä‘Ã¡nh giÃ¡ chÃ­nh xÃ¡c
        text = text.replace("[X_SEP]", " ")
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def compute_bleu(self, reference, generated):
        # TÃ­nh BLEU Score vá»›i phÆ°Æ¡ng phÃ¡p lÃ m mÆ°á»£t Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ tá»‘t hÆ¡n
        reference_tokens = reference.split()
        generated_tokens = generated.split()
        smoothie = SmoothingFunction().method4  # LÃ m mÆ°á»£t BLEU
        bleu_score = sentence_bleu([reference_tokens], generated_tokens, smoothing_function=smoothie)
        return bleu_score

    def compute_rouge(self, reference, generated):
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
        scores = scorer.score(reference, generated)
        return scores

    def save_evaluation(self, strategy_name, bleu_score, rouge_scores):
        # Ghi káº¿t quáº£ Ä‘Ã¡nh giÃ¡ vÃ o file
        with open(self.result_file, "a", encoding="utf-8") as file:
            file.write(f"\n**ÄÃ¡nh giÃ¡ vÄƒn báº£n ({strategy_name}):**\n")
            file.write(f"ğŸ”¹ BLEU Score: {bleu_score:.4f}\n")
            file.write(f"ğŸ”¹ ROUGE-1: {rouge_scores['rouge1'].fmeasure:.4f}\n")
            file.write(f"ğŸ”¹ ROUGE-2: {rouge_scores['rouge2'].fmeasure:.4f}\n")
            file.write(f"ğŸ”¹ ROUGE-L: {rouge_scores['rougeL'].fmeasure:.4f}\n")
            file.write("=" * 80 + "\n")

    def evaluate_text(self, strategy_name, file_path):
        reference_text = self.load_text(self.reference_path)
        generated_text = self.load_text(file_path)

        # LÃ m sáº¡ch vÄƒn báº£n sinh ra
        cleaned_generated_text = self.clean_generated_text(generated_text)

        # TÃ­nh Ä‘iá»ƒm BLEU
        bleu_score = self.compute_bleu(reference_text, cleaned_generated_text)

        # TÃ­nh Ä‘iá»ƒm ROUGE
        rouge_scores = self.compute_rouge(reference_text, cleaned_generated_text)

        # In káº¿t quáº£ ra mÃ n hÃ¬nh
        print(f"\n**ÄÃ¡nh giÃ¡ vÄƒn báº£n ({strategy_name}):**")
        print(f"ğŸ”¹ BLEU Score: {bleu_score:.4f}")
        print(f"ğŸ”¹ ROUGE-1: {rouge_scores['rouge1'].fmeasure:.4f}")
        print(f"ğŸ”¹ ROUGE-2: {rouge_scores['rouge2'].fmeasure:.4f}")
        print(f"ğŸ”¹ ROUGE-L: {rouge_scores['rougeL'].fmeasure:.4f}")
        print("=" * 80)

        # LÆ°u káº¿t quáº£ vÃ o file
        self.save_evaluation(strategy_name, bleu_score, rouge_scores)

    def evaluate_all_methods(self):
        output_dir = os.path.abspath(self.output_dir)

        methods = {
            "1ï¸. Máº·c Ä‘á»‹nh (Beam Search)": os.path.join(output_dir, "Outputs", "final_output_default.txt"),
            "2ï¸. Greedy Search": os.path.join(output_dir, "Outputs", "final_output_greedy.txt"),
            "3ï¸. Beam Search (num_beams=1)": os.path.join(output_dir, "Outputs", "final_output_beam_search.txt"),
            "4ï¸. Top-k Sampling": os.path.join(output_dir, "Outputs", "final_output_top_k.txt"),
            "5ï¸. Top-p Sampling": os.path.join(output_dir, "Outputs", "final_output_top_p.txt")
        }

        for strategy_name, file_path in methods.items():
            if not os.path.exists(file_path):
                print(f"File khÃ´ng tá»“n táº¡i: {file_path}")
            else:
                self.evaluate_text(strategy_name, file_path)

if __name__ == "__main__":
    evaluator = Evaluation()
    evaluator.evaluate_all_methods()
    print("\n**ÄÃ£ lÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡ vÃ o file:** object/evaluation_results.txt\n")
