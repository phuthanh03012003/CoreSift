import torch
from transformers import ProphetNetForConditionalGeneration, ProphetNetTokenizer
from transformers.modeling_outputs import BaseModelOutput

class TextGeneration:
    
    def __init__(self, model_name="microsoft/prophetnet-large-uncased-cnndm"):
        # T·∫£i m√¥ h√¨nh v√† tokenizer
        self.tokenizer = ProphetNetTokenizer.from_pretrained(model_name)
        self.model = ProphetNetForConditionalGeneration.from_pretrained(model_name)

    def load_inputs(self, input_path):
        # T·∫£i d·ªØ li·ªáu t·ª´ Decoder Layer (BaseModelOutput t·ª´ Encoder)
        self.inputs = torch.load(input_path)

        if isinstance(self.inputs, dict) and 'encoder_outputs' in self.inputs:
            print("ƒê√£ t·∫£i d·ªØ li·ªáu t·ª´ Encoder Layer (encoder_outputs, attention_mask, decoder_input_ids).")
            self.encoder_outputs = self.inputs['encoder_outputs']
            self.attention_mask = self.inputs['attention_mask']
            self.decoder_input_ids = self.inputs['decoder_input_ids']

            print("üî∏ Encoder Hidden States:", self.encoder_outputs.last_hidden_state[0, :7])
            print("üî∏ Attention Mask:", self.attention_mask[0, :7])
            print("üî∏ Decoder Input IDs:", self.decoder_input_ids[0, :7])
        else:
            raise ValueError("D·ªØ li·ªáu kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng. C·∫ßn ch·ª©a 'encoder_outputs'.")

    def _prepare_inputs(self):
        # Chu·∫©n b·ªã d·ªØ li·ªáu tr∆∞·ªõc khi sinh vƒÉn b·∫£n
        batch_size, seq_len, _ = self.encoder_outputs.last_hidden_state.shape

        if self.attention_mask.shape[0] != batch_size:
            repeat_factor = batch_size // self.attention_mask.shape[0]
            self.attention_mask = self.attention_mask.repeat(repeat_factor, 1)

        self.attention_mask = self.attention_mask.to(self.model.device)

        self.decoder_input_ids = torch.full(
            (batch_size, 1),
            self.model.config.decoder_start_token_id,
            dtype=torch.long
        ).to(self.model.device)

        self.encoder_outputs = BaseModelOutput(
            last_hidden_state=self.encoder_outputs.last_hidden_state.to(self.model.device)
        )

    def generate_text(self, strategy="beam_search"):
        """
        Sinh vƒÉn b·∫£n theo chi·∫øn l∆∞·ª£c t·ªëi ∆∞u h√≥a:
        - "default": Beam Search m·∫∑c ƒë·ªãnh (num_beams=5)
        - "greedy": Greedy Search (num_beams=1)
        - "beam_search": Beam Search (num_beams=10)
        - "top_k": Top-k Sampling (top_k=50)
        - "top_p": Top-p Sampling (top_p=0.9)
        """
        self._prepare_inputs()

        generation_config = {
            "input_ids": self.decoder_input_ids,
            "encoder_outputs": self.encoder_outputs,
            "attention_mask": self.attention_mask,
            "max_length": 256,
            "min_length": 64,
            "length_penalty": 2.0,
        }

        file_name = "../object/TextGeneration/generated_"
        if strategy == "greedy":
            generation_config["num_beams"] = 1
            file_name += "greedy.pt"

        elif strategy == "beam_search":
            generation_config["num_beams"] = 1
            generation_config["early_stopping"] = True
            file_name += "beam_search.pt"

        elif strategy == "top_k":
            generation_config.update({"do_sample": True, "top_k": 10})
            generation_config["early_stopping"] = True
            file_name += "top_k.pt"

        elif strategy == "top_p":
            generation_config.update({"do_sample": True, "top_p": 0.5})
            generation_config["early_stopping"] = True
            file_name += "top_p.pt"

        else:
            file_name += "default.pt"

        with torch.no_grad():
            generated_ids = self.model.generate(**generation_config)
        
        torch.save(generated_ids, file_name)
        print(f"Token IDs ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o {file_name}")

        strategy_name = {
            "default": "M·∫∑c ƒë·ªãnh - Beam Search",
            "greedy": "Greedy Search",
            "beam_search": "Beam Search (num_beams=5)",
            "top_k": "Top-k Sampling",
            "top_p": "Top-p Sampling"
        }
        print(f"\nToken IDs sinh ra ({strategy_name[strategy]}):\n{generated_ids[0]}")

def show_menu():
    print("\n===== üìú CH·ªåN PH∆Ø∆†NG PH√ÅP SINH VƒÇN B·∫¢N =====")
    print("1Ô∏è. M·∫∑c ƒë·ªãnh (Beam Search)")
    print("2Ô∏è. Greedy Search")
    print("3Ô∏è. Beam Search (num_beams=5)")
    print("4Ô∏è. Top-k Sampling")
    print("5Ô∏è. Top-p Sampling")
    print("Nh·∫•n ph√≠m b·∫•t k·ª≥ ƒë·ªÉ tho√°t")
    print("===========================================")

if __name__ == "__main__":
    text_gen = TextGeneration()
    text_gen.load_inputs("../object/decoder_outputs_default.pt")

    while True:
        show_menu()
        choice = input("Nh·∫≠p l·ª±a ch·ªçn (1-5): ")

        if choice == "1":
            text_gen.generate_text(strategy="default")
        elif choice == "2":
            text_gen.generate_text(strategy="greedy")
        elif choice == "3":
            text_gen.generate_text(strategy="beam_search")
        elif choice == "4":
            text_gen.generate_text(strategy="top_k")
        elif choice == "5":
            text_gen.generate_text(strategy="top_p")
        else:
            print("Tho√°t ch∆∞∆°ng tr√¨nh. T·∫°m bi·ªát!")
            break
