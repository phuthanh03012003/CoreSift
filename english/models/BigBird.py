from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer
import os
import re  # ThÆ° viá»‡n xá»­ lÃ½ biá»ƒu thá»©c chÃ­nh quy

# Load model vÃ  tokenizer
model_name = "google/bigbird-pegasus-large-arxiv"  # MÃ´ hÃ¬nh tá»‘i Æ°u cho tÃ³m táº¯t vÄƒn báº£n dÃ i
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = BigBirdPegasusForConditionalGeneration.from_pretrained(model_name)

# Äá»c ná»™i dung tá»« file input (KHÃ”NG dÃ¹ng encoding="utf-8")
input_file = "../inputs/TamCam.txt"
with open(input_file, "r") as f:
    raw_text = f.read().strip()

# Tokenize vÄƒn báº£n
inputs = tokenizer(
    raw_text,
    return_tensors="pt",
    max_length=4096,  # BigBird há»— trá»£ tá»‘i Ä‘a 4096 token (xá»­ lÃ½ vÄƒn báº£n dÃ i)
    truncation=True
)

# Sinh káº¿t quáº£ tÃ³m táº¯t
summary_ids = model.generate(
    inputs["input_ids"],
    max_length=300,          # Äá»™ dÃ i tá»‘i Ä‘a cá»§a tÃ³m táº¯t
    min_length=100,          # Äá»™ dÃ i tá»‘i thiá»ƒu
    length_penalty=2.0,      # Pháº¡t Ä‘á»™ dÃ i (Æ°u tiÃªn vÄƒn báº£n sÃºc tÃ­ch)
    num_beams=5,             # Beam search Ä‘á»ƒ tÄƒng cháº¥t lÆ°á»£ng káº¿t quáº£
    early_stopping=True      # Dá»«ng sá»›m khi Ä‘áº¡t káº¿t quáº£ tá»‘t
)

# Giáº£i mÃ£ káº¿t quáº£ tÃ³m táº¯t
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# âœ… Xá»­ lÃ½ xuá»‘ng dÃ²ng sau má»—i cÃ¢u
formatted_summary = re.sub(r'([.!?])\s', r'\1\n', summary)

# In káº¿t quáº£ ra mÃ n hÃ¬nh
print("ğŸ“„ Káº¿t quáº£ tÃ³m táº¯t:\n", formatted_summary)

# Ghi káº¿t quáº£ vÃ o file (KHÃ”NG dÃ¹ng encoding="utf-8")
output_file = "../outputs/BigBird-pegasus-large-arxiv.txt"
os.makedirs(os.path.dirname(output_file), exist_ok=True)
with open(output_file, "w") as f:
    f.write(formatted_summary)

print(f"ğŸ“‚ Káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c ghi vÃ o file: {output_file}")