import os
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu
from textblob import TextBlob

class Evaluation:
    def __init__(self, output_dir="./object"):
        self.output_dir = output_dir
        self.reference_path = os.path.join("..", "..", "..", "inputs", "TamCam.txt")
        self.result_file = os.path.join(self.output_dir, "evaluation_results.txt")  

        # XÃ³a ná»™i dung cÅ© náº¿u file Ä‘Ã£ tá»“n táº¡i
        with open(self.result_file, "w", encoding="utf-8") as file:
            file.write("**Tá»”NG Há»¢P Káº¾T QUáº¢ ÄÃNH GIÃ**\n")
            file.write("=" * 80 + "\n")

    def load_text(self, file_path):
        with open(file_path, "r", encoding="utf-8") as file:
            return file.read().strip()

    def compute_bleu(self, reference, generated):
        reference_tokens = reference.split()
        generated_tokens = generated.split()
        bleu_score = sentence_bleu([reference_tokens], generated_tokens)
        return bleu_score

    def compute_rouge(self, reference, generated):
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
        scores = scorer.score(reference, generated)
        return scores

    def grammar_check(self, text):
        blob = TextBlob(text)
        corrected_text = str(blob.correct())
        return corrected_text

    def save_evaluation(self, strategy_name, bleu_score, rouge_scores, corrected_text):
        # Ghi káº¿t quáº£ Ä‘Ã¡nh giÃ¡ vÃ o file
        with open(self.result_file, "a", encoding="utf-8") as file:
            file.write(f"\n**ÄÃ¡nh giÃ¡ vÄƒn báº£n ({strategy_name}):**\n")
            file.write(f"ğŸ”¹ BLEU Score: {bleu_score:.4f}\n")
            file.write(f"ğŸ”¹ ROUGE-1: {rouge_scores['rouge1'].fmeasure:.4f}\n")
            file.write(f"ğŸ”¹ ROUGE-2: {rouge_scores['rouge2'].fmeasure:.4f}\n")
            file.write(f"ğŸ”¹ ROUGE-L: {rouge_scores['rougeL'].fmeasure:.4f}\n\n")
            file.write("**VÄƒn báº£n sau khi sá»­a ngá»¯ phÃ¡p:**\n")
            file.write(corrected_text + "\n")
            file.write("=" * 80 + "\n")

    def evaluate_text(self, strategy_name, file_path):
        reference_text = self.load_text(self.reference_path)
        generated_text = self.load_text(file_path)

        # TÃ­nh Ä‘iá»ƒm BLEU
        bleu_score = self.compute_bleu(reference_text, generated_text)

        # TÃ­nh Ä‘iá»ƒm ROUGE
        rouge_scores = self.compute_rouge(reference_text, generated_text)

        # Kiá»ƒm tra lá»—i ngá»¯ phÃ¡p
        corrected_text = self.grammar_check(generated_text)

        # In káº¿t quáº£ ra mÃ n hÃ¬nh
        print(f"\n**ÄÃ¡nh giÃ¡ vÄƒn báº£n ({strategy_name}):**")
        print(f"ğŸ”¹ BLEU Score: {bleu_score:.4f}")
        print(f"ğŸ”¹ ROUGE-1: {rouge_scores['rouge1'].fmeasure:.4f}")
        print(f"ğŸ”¹ ROUGE-2: {rouge_scores['rouge2'].fmeasure:.4f}")
        print(f"ğŸ”¹ ROUGE-L: {rouge_scores['rougeL'].fmeasure:.4f}")
        print("\n**VÄƒn báº£n sau khi sá»­a ngá»¯ phÃ¡p:**")
        print(corrected_text)
        print("=" * 80)

        # LÆ°u káº¿t quáº£ vÃ o file
        self.save_evaluation(strategy_name, bleu_score, rouge_scores, corrected_text)

    def evaluate_all_methods(self):
        output_dir = os.path.abspath(self.output_dir)

        methods = {
            "1ï¸. Máº·c Ä‘á»‹nh (Beam Search)": os.path.join(output_dir, "Outputs", "final_output_default.txt"),
            "2ï¸. Greedy Search": os.path.join(output_dir, "Outputs", "final_output_greedy.txt"),
            "3ï¸. Beam Search (num_beams=1)": os.path.join(output_dir, "Outputs", "final_output_beam_search.txt"),
            "4ï¸. Top-k Sampling": os.path.join(output_dir, "Outputs", "final_output_top_k.txt"),
            "5ï¸. Top-p Sampling": os.path.join(output_dir, "Outputs", "final_output_top_p.txt")
        }

        for strategy_name, file_path in methods.items():
            if not os.path.exists(file_path):
                print(f"File khÃ´ng tá»“n táº¡i: {file_path}")
            else:
                self.evaluate_text(strategy_name, file_path)

if __name__ == "__main__":
    evaluator = Evaluation()
    evaluator.evaluate_all_methods()
    print("\n**ÄÃ£ lÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡ vÃ o file:** object/evaluation_results.txt\n")
