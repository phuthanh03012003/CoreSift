import torch
from transformers import T5Tokenizer
import re

class PostProcessing:
    def __init__(self, model_name="t5-large"):
        # T·∫£i tokenizer c·ªßa T5 ƒë·ªÉ gi·∫£i m√£ token th√†nh vƒÉn b·∫£n
        self.tokenizer = T5Tokenizer.from_pretrained(model_name)

    def load_generated_tokens(self, token_path):
        # T·∫£i token IDs ƒë√£ sinh ra t·ª´ qu√° tr√¨nh sinh vƒÉn b·∫£n
        self.generated_tokens = torch.load(token_path)
        print(f"\nƒê√£ t·∫£i Token IDs t·ª´ {token_path}.")

    def decode_tokens(self):
        # Gi·∫£i m√£ token th√†nh vƒÉn b·∫£n t·ª± nhi√™n
        decoded_text = self.tokenizer.decode(
            self.generated_tokens[0], skip_special_tokens=True
        )
        return decoded_text

    def clean_text(self, text):
        # 1Ô∏è Thay th·∫ø <n> ho·∫∑c </n> b·∫±ng d·∫•u xu·ªëng d√≤ng
        text = text.replace("<n>", "\n").replace("</n>", "\n")

        # 2Ô∏è T·ª± ƒë·ªông xu·ªëng d√≤ng sau d·∫•u ch·∫•m, d·∫•u h·ªèi, d·∫•u ch·∫•m than
        text = re.sub(r'([.!?])\s+', r'\1\n', text)

        # 3Ô∏è X√≥a kho·∫£ng tr·∫Øng th·ª´a v√† k√Ω t·ª± ƒë·∫∑c bi·ªát
        text = text.strip().replace("  ", " ")

        return text

    def save_final_output(self, text, save_path):
        # L∆∞u vƒÉn b·∫£n ho√†n ch·ªânh v√†o file
        with open(save_path, "w", encoding="utf-8") as file:
            file.write(text)
        print(f"ƒê√£ l∆∞u vƒÉn b·∫£n ho√†n ch·ªânh v√†o {save_path}")

    def process_and_display(self, token_path, save_path, strategy_name):
        # Quy tr√¨nh: Load -> Gi·∫£i m√£ -> L√†m s·∫°ch -> L∆∞u file -> In k·∫øt qu·∫£
        self.load_generated_tokens(token_path)
        decoded_text = self.decode_tokens()
        cleaned_text = self.clean_text(decoded_text)
        self.save_final_output(cleaned_text, save_path)

        # In k·∫øt qu·∫£ ra m√†n h√¨nh
        print(f"\nüìÑ {strategy_name}:")
        print(cleaned_text)
        print("=" * 80)

if __name__ == "__main__":
    post_processor = PostProcessing()

    # Mapping gi·ªØa file sinh token v√† file k·∫øt qu·∫£
    strategy_files = {
        "1Ô∏è. M·∫∑c ƒë·ªãnh (Beam Search)": {
            "token_path": "object/TextGeneration/generated_default.pt",
            "save_path": "object/Outputs/final_output_default.txt"
        },
        "2Ô∏è. Greedy Search": {
            "token_path": "object/TextGeneration/generated_greedy.pt",
            "save_path": "object/Outputs/final_output_greedy.txt"
        },
        "3Ô∏è. Beam Search (num_beams=10)": {
            "token_path": "object/TextGeneration/generated_beam_search.pt",
            "save_path": "object/Outputs/final_output_beam_search.txt"
        },
        "4Ô∏è. Top-k Sampling": {
            "token_path": "object/TextGeneration/generated_top_k.pt",
            "save_path": "object/Outputs/final_output_top_k.txt"
        },
        "5Ô∏è. Top-p Sampling": {
            "token_path": "object/TextGeneration/generated_top_p.pt",
            "save_path": "object/Outputs/final_output_top_p.txt"
        }
    }

    # X·ª≠ l√Ω to√†n b·ªô c√°c ph∆∞∆°ng ph√°p
    for strategy_name, paths in strategy_files.items():
        post_processor.process_and_display(
            token_path=paths["token_path"],
            save_path=paths["save_path"],
            strategy_name=strategy_name
        )

    print("\n ƒê√É HO√ÄN TH√ÄNH X·ª¨ L√ù T·∫§T C·∫¢ C√ÅC PH∆Ø∆†NG PH√ÅP.\n")
