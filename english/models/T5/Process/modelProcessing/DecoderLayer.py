import torch
import torch.nn as nn
from transformers import T5ForConditionalGeneration, T5Tokenizer
from transformers.modeling_outputs import BaseModelOutput

class DecoderLayer:
    def __init__(self, model_name="t5-large"):
        # Táº£i mÃ´ hÃ¬nh vÃ  tokenizer
        self.tokenizer = T5Tokenizer.from_pretrained(model_name)
        self.model = T5ForConditionalGeneration.from_pretrained(model_name)
        self.d_model = self.model.config.d_model

    def load_encoder_outputs(self, encoder_output_path, attention_mask_path):
        # Äá»c Hidden States (BaseModelOutput) vÃ  Attention Mask tá»« Encoder
        self.encoder_hidden_states = torch.load(encoder_output_path)
        self.attention_mask = torch.load(attention_mask_path)

        print("\nÄÃ£ táº£i Encoder Outputs (BaseModelOutput) vÃ  Attention Mask.")
        print("ğŸ”¸ Hidden States:", self.encoder_hidden_states.last_hidden_state[0, :7])
        print("ğŸ”¸ Attention Mask:", self.attention_mask[0, :7])

    def decoding_default(self, save_path="../object/decoder_outputs_default.pt"):
        batch_size = self.encoder_hidden_states.last_hidden_state.shape[0]

        # âš ï¸ ThÃªm prefix "summarize: " Ä‘á»ƒ Ä‘á»‹nh hÆ°á»›ng mÃ´ hÃ¬nh
        decoder_input_text = ["summarize: "] * batch_size
        decoder_input_ids = self.tokenizer(decoder_input_text, return_tensors="pt").input_ids

        print("ğŸ”¹ Batch size:", batch_size)
        print("ğŸ”¹ Decoder input IDs shape:", decoder_input_ids.shape)
        print("ğŸ”¹ Encoder hidden states shape:", self.encoder_hidden_states.last_hidden_state.shape)
        print("ğŸ”¹ Attention mask shape:", self.attention_mask.shape)

        # Thá»±c hiá»‡n decoding máº·c Ä‘á»‹nh
        with torch.no_grad():
            generated_ids = self.model.generate(
                input_ids=decoder_input_ids,
                encoder_outputs=self.encoder_hidden_states,
                attention_mask=self.attention_mask,
                max_length=128,
                num_beams=5,
                early_stopping=True
            )

        print("\nKáº¿t quáº£ tá»« Decoder Layer (Máº¶C Äá»ŠNH):")
        print(generated_ids)
        print("ğŸ”¹ Generated IDs shape:", generated_ids.shape)

        self.save_decoder_outputs(save_path, decoder_input_ids)

    def decoding_custom(self, save_path="../object/decoder_outputs_custom.pt"):
        batch_size = self.encoder_hidden_states.last_hidden_state.shape[0]

        # âš ï¸ ThÃªm prefix "summarize: " Ä‘á»ƒ mÃ´ hÃ¬nh hiá»ƒu nhiá»‡m vá»¥
        decoder_input_text = ["summarize: "] * batch_size
        decoder_input_ids = self.tokenizer(decoder_input_text, return_tensors="pt").input_ids

        # Multi-Head Self-Attention cho Decoder (Masked)
        masked_self_attention = nn.MultiheadAttention(embed_dim=self.d_model, num_heads=8, batch_first=True)

        # Cross-Attention (Káº¿t ná»‘i vá»›i Encoder)
        cross_attention = nn.MultiheadAttention(embed_dim=self.d_model, num_heads=8, batch_first=True)

        # Feed Forward Network (FFN)
        ffn = nn.Sequential(
            nn.Linear(self.d_model, self.d_model * 4),
            nn.ReLU(),
            nn.Linear(self.d_model * 4, self.d_model)
        )

        # Layer Normalization
        layer_norm1 = nn.LayerNorm(self.d_model)
        layer_norm2 = nn.LayerNorm(self.d_model)
        layer_norm3 = nn.LayerNorm(self.d_model)

        # Decoder Layer Thá»§ CÃ´ng
        def decoder_layer(x, encoder_hidden_states, mask=None):
            attn_output, _ = masked_self_attention(x, x, x, attn_mask=mask)
            x = layer_norm1(x + attn_output)

            cross_attn_output, _ = cross_attention(x, encoder_hidden_states, encoder_hidden_states)
            x = layer_norm2(x + cross_attn_output)

            ffn_output = ffn(x)
            x = layer_norm3(x + ffn_output)
            return x

        # Embedding cho Decoder Input
        with torch.no_grad():
            decoder_embeddings = self.model.get_decoder().embed_tokens(decoder_input_ids)

            self.decoder_output_manual = decoder_layer(
                decoder_embeddings,
                self.encoder_hidden_states.last_hidden_state
            )

        print("\nKáº¿t quáº£ tá»« Decoder Layer (Thá»§ cÃ´ng tá»«ng bÆ°á»›c):")
        print(self.decoder_output_manual[0, :7])
        print("ğŸ”¹ KÃ­ch thÆ°á»›c:", self.decoder_output_manual.shape)

        self.save_decoder_outputs(save_path, decoder_input_ids)

    def save_decoder_outputs(self, path, decoder_input_ids):
        data_to_save = {
            "encoder_outputs": self.encoder_hidden_states,
            "attention_mask": self.attention_mask,
            "decoder_input_ids": decoder_input_ids
        }
        torch.save(data_to_save, path)
        print(f"\nÄÃ£ lÆ°u káº¿t quáº£ tá»« Decoder Layer vÃ o {path}")

if __name__ == "__main__":
    decoder = DecoderLayer()
    decoder.load_encoder_outputs("../object/encoder_outputs.pt", "../object/attention_mask.pt")
    decoder.decoding_default()
    decoder.decoding_custom()
