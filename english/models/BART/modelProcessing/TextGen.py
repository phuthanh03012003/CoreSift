import torch
from transformers import BartForConditionalGeneration, BartTokenizer
from transformers.modeling_outputs import BaseModelOutput

class TextGeneration:
    
    def __init__(self, model_name="facebook/bart-large-cnn"):
        # Táº£i mÃ´ hÃ¬nh vÃ  tokenizer
        self.tokenizer = BartTokenizer.from_pretrained(model_name)
        self.model = BartForConditionalGeneration.from_pretrained(model_name)

    def load_inputs(self, input_path):
        # Táº£i dá»¯ liá»‡u tá»« Decoder Layer
        self.inputs = torch.load(input_path)

        # âœ… Kiá»ƒm tra Ä‘á»‹nh dáº¡ng dá»¯ liá»‡u Ä‘Ã£ load
        if isinstance(self.inputs, dict) and 'last_hidden_state' in self.inputs:
            print("âœ… ÄÃ£ táº£i dá»¯ liá»‡u tá»« Decoder Layer (last_hidden_state, attention_mask, decoder_input_ids).")
            self.decoder_hidden_states = self.inputs['last_hidden_state']
            self.attention_mask = self.inputs['attention_mask']
            self.decoder_input_ids = self.inputs['decoder_input_ids']
            
            print("ğŸ” Hidden States:", self.decoder_hidden_states[0, :7])
            print("ğŸ” Attention Mask:", self.attention_mask[0, :7])
            print("ğŸ” Decoder Input IDs:", self.decoder_input_ids[0, :7])
        else:
            raise ValueError("âŒ Äá»‹nh dáº¡ng dá»¯ liá»‡u khÃ´ng há»£p lá»‡. Cáº§n chá»©a 'last_hidden_state', 'attention_mask', vÃ  'decoder_input_ids'.")

    def generate_default(self):
        # 1ï¸âƒ£. Máº·c Ä‘á»‹nh cá»§a BART (Beam Search vá»›i num_beams=5)
        encoder_outputs = BaseModelOutput(last_hidden_state=self.decoder_hidden_states)

        with torch.no_grad():
            generated_ids = self.model.generate(
                input_ids=self.decoder_input_ids,
                attention_mask=self.attention_mask,
                encoder_outputs=encoder_outputs,  # âœ… ThÃªm encoder_outputs
                max_length=50,
                min_length=10  # âœ… TrÃ¡nh cáº£nh bÃ¡o
            )
        output = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
        print("\n1ï¸âƒ£. VÄƒn báº£n sinh ra (Máº·c Ä‘á»‹nh - Beam Search):\n", output)

    def generate_greedy(self):
        # 2ï¸âƒ£a. Greedy Search
        encoder_outputs = BaseModelOutput(last_hidden_state=self.decoder_hidden_states)

        with torch.no_grad():
            generated_ids = self.model.generate(
                input_ids=self.decoder_input_ids,
                attention_mask=self.attention_mask,
                encoder_outputs=encoder_outputs,  # âœ… ThÃªm encoder_outputs
                max_length=50,
                num_beams=1
            )
        output = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
        print("\n2ï¸âƒ£a. VÄƒn báº£n sinh ra (Greedy Search):\n", output)

    def generate_beam_search(self):
        # 2ï¸âƒ£b. Beam Search vá»›i num_beams=10
        encoder_outputs = BaseModelOutput(last_hidden_state=self.decoder_hidden_states)

        with torch.no_grad():
            generated_ids = self.model.generate(
                input_ids=self.decoder_input_ids,
                attention_mask=self.attention_mask,
                encoder_outputs=encoder_outputs,  # âœ… ThÃªm encoder_outputs
                max_length=50,
                num_beams=10,
                early_stopping=True
            )
        output = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
        print("\n2ï¸âƒ£b. VÄƒn báº£n sinh ra (Beam Search num_beams=10):\n", output)

    def generate_top_k_sampling(self):
        # 2ï¸âƒ£c. Top-k Sampling
        encoder_outputs = BaseModelOutput(last_hidden_state=self.decoder_hidden_states)

        with torch.no_grad():
            generated_ids = self.model.generate(
                input_ids=self.decoder_input_ids,
                attention_mask=self.attention_mask,
                encoder_outputs=encoder_outputs,  # âœ… ThÃªm encoder_outputs
                max_length=50,
                do_sample=True,
                top_k=50
            )
        output = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
        print("\n2ï¸âƒ£c. VÄƒn báº£n sinh ra (Top-k Sampling):\n", output)

    def generate_top_p_sampling(self):
        # 2ï¸âƒ£d. Top-p (Nucleus) Sampling
        encoder_outputs = BaseModelOutput(last_hidden_state=self.decoder_hidden_states)

        with torch.no_grad():
            generated_ids = self.model.generate(
                input_ids=self.decoder_input_ids,
                attention_mask=self.attention_mask,
                encoder_outputs=encoder_outputs,  # âœ… ThÃªm encoder_outputs
                max_length=50,
                do_sample=True,
                top_p=0.9
            )
        output = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
        print("\n2ï¸âƒ£d. VÄƒn báº£n sinh ra (Top-p Sampling):\n", output)


if __name__ == "__main__":
    text_gen = TextGeneration()

    # âœ… Load dá»¯ liá»‡u tá»« Decoder Layer (chá»©a last_hidden_state, attention_mask, decoder_input_ids)
    text_gen.load_inputs("../object/decoder_outputs_default.pt")

    # ğŸ”¥ Thá»±c hiá»‡n sinh vÄƒn báº£n vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c nhau
    text_gen.generate_default()       # 1ï¸âƒ£ Máº·c Ä‘á»‹nh (Beam Search)
    text_gen.generate_greedy()        # 2ï¸âƒ£a. Greedy Search
    text_gen.generate_beam_search()   # 2ï¸âƒ£b. Beam Search num_beams=10
    text_gen.generate_top_k_sampling()  # 2ï¸âƒ£c. Top-k Sampling
    text_gen.generate_top_p_sampling()  # 2ï¸âƒ£d. Top-p Sampling
